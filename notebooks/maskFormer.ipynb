{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from detectron2.config import configurable\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.modeling import META_ARCH_REGISTRY, build_backbone, build_sem_seg_head\n",
    "from detectron2.modeling.backbone import Backbone\n",
    "from detectron2.modeling.postprocessing import sem_seg_postprocess\n",
    "from detectron2.structures import ImageList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@META_ARCH_REGISTRY\n",
    "class MaskFormer(nn.Module):\n",
    "    @configurable\n",
    "    def __init__(self, \n",
    "                 *,backbone:Backbone,\n",
    "                 sem_seg_head:nn.Module,\n",
    "                 criterion:nn.Module,\n",
    "                 num_queries:int,\n",
    "                 panoptic_on:bool,\n",
    "                 object_mask_threshold:float,\n",
    "                 overlap_threshold:float,\n",
    "                 metadata,\n",
    "                 size_divisibility:int,\n",
    "                 sem_seg_postprocess_before_inference:bool,\n",
    "                 pixel_mean:Tuple[float],\n",
    "                 pixel_std:Tuple[float]) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bacbone: a backone module, must follow detectron2's bacbone interface. SwinTransformer or Resnet?\n",
    "            sem_seg_head: a module that predicts semantic segmentation from backbone features.\n",
    "            criterion: a module that define the loss. Maybe can change to Non mutual exclusive loss?\n",
    "            num_queries: int, number of queries.\n",
    "            panoptic_on: bool, whether to output panoptic segmentation prediction or not.\n",
    "            object_mask_threshold: float, threshold to filter query based on classification score for panoptic segmentation inference.\n",
    "            overlap_threshold: overlap threshold used in general inference for panoptic segmentation.\n",
    "            metadata: dataset meta, get `thing` and `stuff` category names for panoptic segmentation inference.\n",
    "            size_divisibility: Some backbones require the input height and width to be divisible by a specific integer. We can use this override such requirement.\n",
    "            sem_seg_postprocess_before_inference. whether to resize the prediction back to original input size before semantic segmentation inference or after.\n",
    "            pixel_mean, pixel_std: list or tuple with channels element, representing the per channel mean and std to be used to normalize image.\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.sem_seg_head = sem_seg_head\n",
    "        self.criterion = criterion\n",
    "        self.num_queries = num_queries\n",
    "        self.overlap_threshold = overlap_threshold\n",
    "        self.panoptic_on = panoptic_on\n",
    "        self.object_mask_threshold = object_mask_threshold\n",
    "        self.metadata = metadata\n",
    "        if size_divisibility < 0:\n",
    "            size_divisibility = self.backbone.size_divisibility\n",
    "        self.size_divisibility = size_divisibility\n",
    "        self.sem_seg_postprocess_before_inference = sem_seg_postprocess_before_inference\n",
    "        self.register_buffer(\n",
    "            \"pixel_mean\",torch.Tensor(pixel_mean).view(-1,1,1),False\n",
    "        )\n",
    "        self.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg):\n",
    "        backbone = build_backbone(cfg)\n",
    "        sem_seg_head = build_sem_seg_head(cfg=cfg, input_shape=backbone.output_shape())\n",
    "\n",
    "        #Loss parameters:\n",
    "        deep_supervision = cfg.MODEL.MASK_FORMER.DEEP_SUPERVISION\n",
    "        no_object_weight = cfg.MODEL.MASK_FORMER.NO_OBJECT_WEIGHT\n",
    "        dice_weight = cfg.MODEL.MASK_FORMER.DICE_WEIGHT\n",
    "        mask_weight = cfg.MODEL.MASK_FORMER.MASK_WEIGHT\n",
    "        \n",
    "\n",
    "        # # building criterion\n",
    "        # matcher = HungarianMatcher(\n",
    "        #     cost_class=1,\n",
    "        #     cost_mask=mask_weight,\n",
    "        #     cost_dice=dice_weight,\n",
    "        # )\n",
    "\n",
    "        weight_dict = {\"loss_ce\": 1, \"loss_mask\": mask_weight, \"loss_dice\": dice_weight}\n",
    "        if deep_supervision:\n",
    "            dec_layers = cfg.MODEL.MASK_FORMER.DEC_LAYERS\n",
    "            aux_weight_dict = {}\n",
    "            for i in range(dec_layers - 1):\n",
    "                aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n",
    "            weight_dict.update(aux_weight_dict)\n",
    "\n",
    "        losses = [\"labels\",\"masks\"]\n",
    "\n",
    "        # criterion = SetCriterion(\n",
    "        #     sem_seg_head.num_classes,\n",
    "        #     matcher=matcher,\n",
    "        #     weight_dict=weight_dict,\n",
    "        #     eos_coef=no_object_weight,\n",
    "        #     losses=losses,\n",
    "        # )\n",
    "\n",
    "        return {\n",
    "            \"backbone\": backbone,\n",
    "            \"sem_seg_head\": sem_seg_head,\n",
    "            \"criterion\": criterion,\n",
    "            \"num_queries\": cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES,\n",
    "            \"panoptic_on\": cfg.MODEL.MASK_FORMER.TEST.PANOPTIC_ON,\n",
    "            \"object_mask_threshold\": cfg.MODEL.MASK_FORMER.TEST.OBJECT_MASK_THRESHOLD,\n",
    "            \"overlap_threshold\": cfg.MODEL.MASK_FORMER.TEST.OVERLAP_THRESHOLD,\n",
    "            \"metadata\": MetadataCatalog.get(cfg.DATASETS.TRAIN[0]),\n",
    "            \"size_divisibility\": cfg.MODEL.MASK_FORMER.SIZE_DIVISIBILITY,\n",
    "            \"sem_seg_postprocess_before_inference\": (\n",
    "                cfg.MODEL.MASK_FORMER.TEST.SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\n",
    "                or cfg.MODEL.MASK_FORMER.TEST.PANOPTIC_ON\n",
    "            ),\n",
    "            \"pixel_mean\": cfg.MODEL.PIXEL_MEAN,\n",
    "            \"pixel_std\": cfg.MODEL.PIXEL_STD,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.pixel_mean.device\n",
    "\n",
    "    def forward(self, batched_inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batched_inputs: a list, batched outputs of DatasetMapper\n",
    "            Each item in the list contains the inputs for one image.\n",
    "            For now, each item in the list is a dict that contains:\n",
    "                * image: Tensor in (C, H, W) format.\n",
    "                * instances: per-region ground truth\n",
    "                * Other information that included in the original dicts, such as \n",
    "                * height, width\n",
    "\n",
    "            Returns:\n",
    "                list[dict]:\n",
    "                    each dict has the result for one image. The dict contains the following keys:\n",
    "                    * \"sem_seg\":\n",
    "                        A tensor that represents the per-pixel segmentation predicted by the head.\n",
    "                        The prediction has shape KxHxW that represent the logits of the each class for each pixel.\n",
    "                    * \"panoptic_seg\"\n",
    "                        A tuple that represent panoptic output\n",
    "                        panoptic seg (Tensor) of shape (height, width)\n",
    "                        segments_info (list[dict]): Describe each segment in `panoptic_seg`.\n",
    "                            Each dict contains keys \"id\", \"category_id\", \"isthing\".\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        images = [x['image'].to(self.device) for x in batched_inputs]\n",
    "        images = [(x - self.pixel_mean) / self.pixel_std for x in images]\n",
    "        images = ImageList.from_tensors(images, self.size_divisibility)\n",
    "\n",
    "        features = self.backbone(images.tensor)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu, Yutong Lin, Yixuan Wei\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# Modified by Bowen Cheng from https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation/blob/main/mmseg/models/backbones/swin_transformer.py\n",
    "# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "from detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = (\n",
    "        x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    )\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(\n",
    "        B, H // window_size, W // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = (\n",
    "            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        )  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(\n",
    "            1, 2, 0\n",
    "        ).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Forward function.\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.view(-1)\n",
    "        ].view(\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            -1,\n",
    "        )  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(\n",
    "            2, 0, 1\n",
    "        ).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n",
    "                1\n",
    "            ).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"Swin Transformer Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert (\n",
    "            0 <= self.shift_size < self.window_size\n",
    "        ), \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        \"\"\"Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "            mask_matrix: Attention mask for cyclic shift.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.H, self.W\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(\n",
    "                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n",
    "            )\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(\n",
    "            shifted_x, self.window_size\n",
    "        )  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(\n",
    "            -1, self.window_size * self.window_size, C\n",
    "        )  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(\n",
    "            x_windows, mask=attn_mask\n",
    "        )  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(\n",
    "                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n",
    "            )\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"Patch Merging Layer\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of feature channels\n",
    "        depth (int): Depths of this stage.\n",
    "        num_heads (int): Number of attention head.\n",
    "        window_size (int): Local window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i]\n",
    "                    if isinstance(drop_path, list)\n",
    "                    else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate attention mask for SW-MSA\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (\n",
    "            slice(0, -self.window_size),\n",
    "            slice(-self.window_size, -self.shift_size),\n",
    "            slice(-self.shift_size, None),\n",
    "        )\n",
    "        w_slices = (\n",
    "            slice(0, -self.window_size),\n",
    "            slice(-self.window_size, -self.shift_size),\n",
    "            slice(-self.shift_size, None),\n",
    "        )\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(\n",
    "            img_mask, self.window_size\n",
    "        )  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n",
    "            attn_mask == 0, float(0.0)\n",
    "        )\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, attn_mask)\n",
    "            else:\n",
    "                x = blk(x, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"Swin Transformer backbone.\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "    Args:\n",
    "        pretrain_img_size (int): Input image size for training the pretrained model,\n",
    "            used in absolute postion embedding. Default 224.\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        depths (tuple[int]): Depths of each Swin Transformer stage.\n",
    "        num_heads (tuple[int]): Number of attention head of each stage.\n",
    "        window_size (int): Window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop_rate (float): Dropout rate.\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0.\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False.\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True.\n",
    "        out_indices (Sequence[int]): Output from which stages.\n",
    "        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n",
    "            -1 means not freezing any parameters.\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrain_img_size=224,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=7,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.2,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        ape=False,\n",
    "        patch_norm=True,\n",
    "        out_indices=(0, 1, 2, 3),\n",
    "        norm_indices=None,\n",
    "        frozen_stages=-1,\n",
    "        use_checkpoint=False,\n",
    "        projection=False,\n",
    "        project_dim=256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrain_img_size = pretrain_img_size\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.out_indices = out_indices\n",
    "        self.norm_indices = norm_indices if norm_indices is not None else out_indices\n",
    "        self.frozen_stages = frozen_stages\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,\n",
    "        )\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            pretrain_img_size = to_2tuple(pretrain_img_size)\n",
    "            patch_size = to_2tuple(patch_size)\n",
    "            patches_resolution = [\n",
    "                pretrain_img_size[0] // patch_size[0],\n",
    "                pretrain_img_size[1] // patch_size[1],\n",
    "            ]\n",
    "\n",
    "            self.absolute_pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1])\n",
    "            )\n",
    "            trunc_normal_(self.absolute_pos_embed, std=0.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2 ** i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n",
    "        self.num_features = num_features\n",
    "\n",
    "        # add a norm layer for each output\n",
    "        for i_layer in self.norm_indices:\n",
    "            if i_layer >= len(self.num_features):\n",
    "                continue\n",
    "            layer = norm_layer(num_features[i_layer])\n",
    "            layer_name = f\"norm{i_layer}\"\n",
    "            self.add_module(layer_name, layer)\n",
    "        # add projector head\n",
    "        self.projection = projection\n",
    "        if projection:\n",
    "            self.project_dim = project_dim\n",
    "            self.norm = norm_layer(self.num_features[-1])\n",
    "            self.projector = nn.Linear(self.num_features[-1], project_dim, bias=False)\n",
    "        self._freeze_stages()\n",
    "\n",
    "    def _freeze_stages(self):\n",
    "        if self.frozen_stages >= 0:\n",
    "            self.patch_embed.eval()\n",
    "            for param in self.patch_embed.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 1 and self.ape:\n",
    "            self.absolute_pos_embed.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 2:\n",
    "            self.pos_drop.eval()\n",
    "            for i in range(0, self.frozen_stages - 1):\n",
    "                m = self.layers[i]\n",
    "                m.eval()\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=0.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        Wh, Ww = x.size(2), x.size(3)\n",
    "        if self.ape:\n",
    "            # interpolate the position embedding to the corresponding size\n",
    "            absolute_pos_embed = F.interpolate(\n",
    "                self.absolute_pos_embed, size=(Wh, Ww), mode=\"bicubic\"\n",
    "            )\n",
    "            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww C\n",
    "        else:\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        outs = {}\n",
    "        for i in range(self.num_layers):\n",
    "            layer = self.layers[i]\n",
    "            x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "\n",
    "            if i in self.out_indices:\n",
    "                if i in self.norm_indices:\n",
    "                    norm_layer = getattr(self, f\"norm{i}\")\n",
    "                    x_out = norm_layer(x_out)\n",
    "                out = (\n",
    "                    x_out.view(-1, H, W, self.num_features[i])\n",
    "                    .permute(0, 3, 1, 2)\n",
    "                    .contiguous()\n",
    "                )\n",
    "                outs[\"res{}\".format(i + 2)] = out\n",
    "        if self.projection:\n",
    "            x_out = self.norm(x_out)\n",
    "            x_out = x_out.view(-1, H, W, self.num_features[-1]).contiguous()\n",
    "            outs[\"fc\"] = self.projector(x_out).permute(0, 3, 1, 2)\n",
    "\n",
    "        return outs\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n",
    "        super(SwinTransformer, self).train(mode)\n",
    "        self._freeze_stages()\n",
    "\n",
    "\n",
    "@BACKBONE_REGISTRY.register()\n",
    "class D2SwinTransformer(SwinTransformer, Backbone):\n",
    "    def __init__(self, cfg, input_shape):\n",
    "\n",
    "        pretrain_img_size = cfg.MODEL.SWIN.PRETRAIN_IMG_SIZE\n",
    "        patch_size = cfg.MODEL.SWIN.PATCH_SIZE\n",
    "        in_chans = 3\n",
    "        embed_dim = cfg.MODEL.SWIN.EMBED_DIM\n",
    "        depths = cfg.MODEL.SWIN.DEPTHS\n",
    "        num_heads = cfg.MODEL.SWIN.NUM_HEADS\n",
    "        window_size = cfg.MODEL.SWIN.WINDOW_SIZE\n",
    "        mlp_ratio = cfg.MODEL.SWIN.MLP_RATIO\n",
    "        qkv_bias = cfg.MODEL.SWIN.QKV_BIAS\n",
    "        qk_scale = cfg.MODEL.SWIN.QK_SCALE\n",
    "        drop_rate = cfg.MODEL.SWIN.DROP_RATE\n",
    "        attn_drop_rate = cfg.MODEL.SWIN.ATTN_DROP_RATE\n",
    "        drop_path_rate = cfg.MODEL.SWIN.DROP_PATH_RATE\n",
    "        norm_layer = nn.LayerNorm\n",
    "        ape = cfg.MODEL.SWIN.APE\n",
    "        patch_norm = cfg.MODEL.SWIN.PATCH_NORM\n",
    "        norm_indices = cfg.MODEL.SWIN.NORM_INDICES\n",
    "        projection = cfg.MODEL.SWIN.PROJECTION\n",
    "        project_dim = cfg.MODEL.SWIN.PROJECT_DIM\n",
    "        super().__init__(\n",
    "            pretrain_img_size,\n",
    "            patch_size,\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            depths,\n",
    "            num_heads,\n",
    "            window_size,\n",
    "            mlp_ratio,\n",
    "            qkv_bias,\n",
    "            qk_scale,\n",
    "            drop_rate,\n",
    "            attn_drop_rate,\n",
    "            drop_path_rate,\n",
    "            norm_layer,\n",
    "            ape,\n",
    "            patch_norm,\n",
    "            norm_indices=norm_indices,\n",
    "            projection=projection,\n",
    "            project_dim=project_dim,\n",
    "        )\n",
    "\n",
    "        self._out_features = cfg.MODEL.SWIN.OUT_FEATURES\n",
    "\n",
    "        self._out_feature_strides = {\n",
    "            \"res2\": 4,\n",
    "            \"res3\": 8,\n",
    "            \"res4\": 16,\n",
    "            \"res5\": 32,\n",
    "            \"fc\": 32,\n",
    "        }\n",
    "        self._out_feature_channels = {\n",
    "            \"res2\": self.num_features[0],\n",
    "            \"res3\": self.num_features[1],\n",
    "            \"res4\": self.num_features[2],\n",
    "            \"res5\": self.num_features[3],\n",
    "            \"fc\": self.num_features[3],\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\n",
    "        Returns:\n",
    "            dict[str->Tensor]: names and the corresponding features\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            x.dim() == 4\n",
    "        ), f\"SwinTransformer takes an input of shape (N, C, H, W). Got {x.shape} instead!\"\n",
    "        outputs = {}\n",
    "        y = super().forward(x)\n",
    "        for k in y.keys():\n",
    "            if k in self._out_features:\n",
    "                outputs[k] = y[k]\n",
    "        return outputs\n",
    "\n",
    "    def output_shape(self):\n",
    "        return {\n",
    "            name: ShapeSpec(\n",
    "                channels=self._out_feature_channels[name],\n",
    "                stride=self._out_feature_strides[name],\n",
    "            )\n",
    "            for name in self._out_features\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def size_divisibility(self):\n",
    "        return 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/workspace/models/ovseg_swinbase_vitL14_ft_mpt.pth\", map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['backbone.patch_embed.proj.weight', 'backbone.patch_embed.proj.bias', 'backbone.patch_embed.norm.weight', 'backbone.patch_embed.norm.bias', 'backbone.layers.0.blocks.0.norm1.weight', 'backbone.layers.0.blocks.0.norm1.bias', 'backbone.layers.0.blocks.0.attn.relative_position_bias_table', 'backbone.layers.0.blocks.0.attn.relative_position_index', 'backbone.layers.0.blocks.0.attn.qkv.weight', 'backbone.layers.0.blocks.0.attn.qkv.bias', 'backbone.layers.0.blocks.0.attn.proj.weight', 'backbone.layers.0.blocks.0.attn.proj.bias', 'backbone.layers.0.blocks.0.norm2.weight', 'backbone.layers.0.blocks.0.norm2.bias', 'backbone.layers.0.blocks.0.mlp.fc1.weight', 'backbone.layers.0.blocks.0.mlp.fc1.bias', 'backbone.layers.0.blocks.0.mlp.fc2.weight', 'backbone.layers.0.blocks.0.mlp.fc2.bias', 'backbone.layers.0.blocks.1.norm1.weight', 'backbone.layers.0.blocks.1.norm1.bias', 'backbone.layers.0.blocks.1.attn.relative_position_bias_table', 'backbone.layers.0.blocks.1.attn.relative_position_index', 'backbone.layers.0.blocks.1.attn.qkv.weight', 'backbone.layers.0.blocks.1.attn.qkv.bias', 'backbone.layers.0.blocks.1.attn.proj.weight', 'backbone.layers.0.blocks.1.attn.proj.bias', 'backbone.layers.0.blocks.1.norm2.weight', 'backbone.layers.0.blocks.1.norm2.bias', 'backbone.layers.0.blocks.1.mlp.fc1.weight', 'backbone.layers.0.blocks.1.mlp.fc1.bias', 'backbone.layers.0.blocks.1.mlp.fc2.weight', 'backbone.layers.0.blocks.1.mlp.fc2.bias', 'backbone.layers.0.downsample.reduction.weight', 'backbone.layers.0.downsample.norm.weight', 'backbone.layers.0.downsample.norm.bias', 'backbone.layers.1.blocks.0.norm1.weight', 'backbone.layers.1.blocks.0.norm1.bias', 'backbone.layers.1.blocks.0.attn.relative_position_bias_table', 'backbone.layers.1.blocks.0.attn.relative_position_index', 'backbone.layers.1.blocks.0.attn.qkv.weight', 'backbone.layers.1.blocks.0.attn.qkv.bias', 'backbone.layers.1.blocks.0.attn.proj.weight', 'backbone.layers.1.blocks.0.attn.proj.bias', 'backbone.layers.1.blocks.0.norm2.weight', 'backbone.layers.1.blocks.0.norm2.bias', 'backbone.layers.1.blocks.0.mlp.fc1.weight', 'backbone.layers.1.blocks.0.mlp.fc1.bias', 'backbone.layers.1.blocks.0.mlp.fc2.weight', 'backbone.layers.1.blocks.0.mlp.fc2.bias', 'backbone.layers.1.blocks.1.norm1.weight', 'backbone.layers.1.blocks.1.norm1.bias', 'backbone.layers.1.blocks.1.attn.relative_position_bias_table', 'backbone.layers.1.blocks.1.attn.relative_position_index', 'backbone.layers.1.blocks.1.attn.qkv.weight', 'backbone.layers.1.blocks.1.attn.qkv.bias', 'backbone.layers.1.blocks.1.attn.proj.weight', 'backbone.layers.1.blocks.1.attn.proj.bias', 'backbone.layers.1.blocks.1.norm2.weight', 'backbone.layers.1.blocks.1.norm2.bias', 'backbone.layers.1.blocks.1.mlp.fc1.weight', 'backbone.layers.1.blocks.1.mlp.fc1.bias', 'backbone.layers.1.blocks.1.mlp.fc2.weight', 'backbone.layers.1.blocks.1.mlp.fc2.bias', 'backbone.layers.1.downsample.reduction.weight', 'backbone.layers.1.downsample.norm.weight', 'backbone.layers.1.downsample.norm.bias', 'backbone.layers.2.blocks.0.norm1.weight', 'backbone.layers.2.blocks.0.norm1.bias', 'backbone.layers.2.blocks.0.attn.relative_position_bias_table', 'backbone.layers.2.blocks.0.attn.relative_position_index', 'backbone.layers.2.blocks.0.attn.qkv.weight', 'backbone.layers.2.blocks.0.attn.qkv.bias', 'backbone.layers.2.blocks.0.attn.proj.weight', 'backbone.layers.2.blocks.0.attn.proj.bias', 'backbone.layers.2.blocks.0.norm2.weight', 'backbone.layers.2.blocks.0.norm2.bias', 'backbone.layers.2.blocks.0.mlp.fc1.weight', 'backbone.layers.2.blocks.0.mlp.fc1.bias', 'backbone.layers.2.blocks.0.mlp.fc2.weight', 'backbone.layers.2.blocks.0.mlp.fc2.bias', 'backbone.layers.2.blocks.1.norm1.weight', 'backbone.layers.2.blocks.1.norm1.bias', 'backbone.layers.2.blocks.1.attn.relative_position_bias_table', 'backbone.layers.2.blocks.1.attn.relative_position_index', 'backbone.layers.2.blocks.1.attn.qkv.weight', 'backbone.layers.2.blocks.1.attn.qkv.bias', 'backbone.layers.2.blocks.1.attn.proj.weight', 'backbone.layers.2.blocks.1.attn.proj.bias', 'backbone.layers.2.blocks.1.norm2.weight', 'backbone.layers.2.blocks.1.norm2.bias', 'backbone.layers.2.blocks.1.mlp.fc1.weight', 'backbone.layers.2.blocks.1.mlp.fc1.bias', 'backbone.layers.2.blocks.1.mlp.fc2.weight', 'backbone.layers.2.blocks.1.mlp.fc2.bias', 'backbone.layers.2.blocks.2.norm1.weight', 'backbone.layers.2.blocks.2.norm1.bias', 'backbone.layers.2.blocks.2.attn.relative_position_bias_table', 'backbone.layers.2.blocks.2.attn.relative_position_index', 'backbone.layers.2.blocks.2.attn.qkv.weight', 'backbone.layers.2.blocks.2.attn.qkv.bias', 'backbone.layers.2.blocks.2.attn.proj.weight', 'backbone.layers.2.blocks.2.attn.proj.bias', 'backbone.layers.2.blocks.2.norm2.weight', 'backbone.layers.2.blocks.2.norm2.bias', 'backbone.layers.2.blocks.2.mlp.fc1.weight', 'backbone.layers.2.blocks.2.mlp.fc1.bias', 'backbone.layers.2.blocks.2.mlp.fc2.weight', 'backbone.layers.2.blocks.2.mlp.fc2.bias', 'backbone.layers.2.blocks.3.norm1.weight', 'backbone.layers.2.blocks.3.norm1.bias', 'backbone.layers.2.blocks.3.attn.relative_position_bias_table', 'backbone.layers.2.blocks.3.attn.relative_position_index', 'backbone.layers.2.blocks.3.attn.qkv.weight', 'backbone.layers.2.blocks.3.attn.qkv.bias', 'backbone.layers.2.blocks.3.attn.proj.weight', 'backbone.layers.2.blocks.3.attn.proj.bias', 'backbone.layers.2.blocks.3.norm2.weight', 'backbone.layers.2.blocks.3.norm2.bias', 'backbone.layers.2.blocks.3.mlp.fc1.weight', 'backbone.layers.2.blocks.3.mlp.fc1.bias', 'backbone.layers.2.blocks.3.mlp.fc2.weight', 'backbone.layers.2.blocks.3.mlp.fc2.bias', 'backbone.layers.2.blocks.4.norm1.weight', 'backbone.layers.2.blocks.4.norm1.bias', 'backbone.layers.2.blocks.4.attn.relative_position_bias_table', 'backbone.layers.2.blocks.4.attn.relative_position_index', 'backbone.layers.2.blocks.4.attn.qkv.weight', 'backbone.layers.2.blocks.4.attn.qkv.bias', 'backbone.layers.2.blocks.4.attn.proj.weight', 'backbone.layers.2.blocks.4.attn.proj.bias', 'backbone.layers.2.blocks.4.norm2.weight', 'backbone.layers.2.blocks.4.norm2.bias', 'backbone.layers.2.blocks.4.mlp.fc1.weight', 'backbone.layers.2.blocks.4.mlp.fc1.bias', 'backbone.layers.2.blocks.4.mlp.fc2.weight', 'backbone.layers.2.blocks.4.mlp.fc2.bias', 'backbone.layers.2.blocks.5.norm1.weight', 'backbone.layers.2.blocks.5.norm1.bias', 'backbone.layers.2.blocks.5.attn.relative_position_bias_table', 'backbone.layers.2.blocks.5.attn.relative_position_index', 'backbone.layers.2.blocks.5.attn.qkv.weight', 'backbone.layers.2.blocks.5.attn.qkv.bias', 'backbone.layers.2.blocks.5.attn.proj.weight', 'backbone.layers.2.blocks.5.attn.proj.bias', 'backbone.layers.2.blocks.5.norm2.weight', 'backbone.layers.2.blocks.5.norm2.bias', 'backbone.layers.2.blocks.5.mlp.fc1.weight', 'backbone.layers.2.blocks.5.mlp.fc1.bias', 'backbone.layers.2.blocks.5.mlp.fc2.weight', 'backbone.layers.2.blocks.5.mlp.fc2.bias', 'backbone.layers.2.blocks.6.norm1.weight', 'backbone.layers.2.blocks.6.norm1.bias', 'backbone.layers.2.blocks.6.attn.relative_position_bias_table', 'backbone.layers.2.blocks.6.attn.relative_position_index', 'backbone.layers.2.blocks.6.attn.qkv.weight', 'backbone.layers.2.blocks.6.attn.qkv.bias', 'backbone.layers.2.blocks.6.attn.proj.weight', 'backbone.layers.2.blocks.6.attn.proj.bias', 'backbone.layers.2.blocks.6.norm2.weight', 'backbone.layers.2.blocks.6.norm2.bias', 'backbone.layers.2.blocks.6.mlp.fc1.weight', 'backbone.layers.2.blocks.6.mlp.fc1.bias', 'backbone.layers.2.blocks.6.mlp.fc2.weight', 'backbone.layers.2.blocks.6.mlp.fc2.bias', 'backbone.layers.2.blocks.7.norm1.weight', 'backbone.layers.2.blocks.7.norm1.bias', 'backbone.layers.2.blocks.7.attn.relative_position_bias_table', 'backbone.layers.2.blocks.7.attn.relative_position_index', 'backbone.layers.2.blocks.7.attn.qkv.weight', 'backbone.layers.2.blocks.7.attn.qkv.bias', 'backbone.layers.2.blocks.7.attn.proj.weight', 'backbone.layers.2.blocks.7.attn.proj.bias', 'backbone.layers.2.blocks.7.norm2.weight', 'backbone.layers.2.blocks.7.norm2.bias', 'backbone.layers.2.blocks.7.mlp.fc1.weight', 'backbone.layers.2.blocks.7.mlp.fc1.bias', 'backbone.layers.2.blocks.7.mlp.fc2.weight', 'backbone.layers.2.blocks.7.mlp.fc2.bias', 'backbone.layers.2.blocks.8.norm1.weight', 'backbone.layers.2.blocks.8.norm1.bias', 'backbone.layers.2.blocks.8.attn.relative_position_bias_table', 'backbone.layers.2.blocks.8.attn.relative_position_index', 'backbone.layers.2.blocks.8.attn.qkv.weight', 'backbone.layers.2.blocks.8.attn.qkv.bias', 'backbone.layers.2.blocks.8.attn.proj.weight', 'backbone.layers.2.blocks.8.attn.proj.bias', 'backbone.layers.2.blocks.8.norm2.weight', 'backbone.layers.2.blocks.8.norm2.bias', 'backbone.layers.2.blocks.8.mlp.fc1.weight', 'backbone.layers.2.blocks.8.mlp.fc1.bias', 'backbone.layers.2.blocks.8.mlp.fc2.weight', 'backbone.layers.2.blocks.8.mlp.fc2.bias', 'backbone.layers.2.blocks.9.norm1.weight', 'backbone.layers.2.blocks.9.norm1.bias', 'backbone.layers.2.blocks.9.attn.relative_position_bias_table', 'backbone.layers.2.blocks.9.attn.relative_position_index', 'backbone.layers.2.blocks.9.attn.qkv.weight', 'backbone.layers.2.blocks.9.attn.qkv.bias', 'backbone.layers.2.blocks.9.attn.proj.weight', 'backbone.layers.2.blocks.9.attn.proj.bias', 'backbone.layers.2.blocks.9.norm2.weight', 'backbone.layers.2.blocks.9.norm2.bias', 'backbone.layers.2.blocks.9.mlp.fc1.weight', 'backbone.layers.2.blocks.9.mlp.fc1.bias', 'backbone.layers.2.blocks.9.mlp.fc2.weight', 'backbone.layers.2.blocks.9.mlp.fc2.bias', 'backbone.layers.2.blocks.10.norm1.weight', 'backbone.layers.2.blocks.10.norm1.bias', 'backbone.layers.2.blocks.10.attn.relative_position_bias_table', 'backbone.layers.2.blocks.10.attn.relative_position_index', 'backbone.layers.2.blocks.10.attn.qkv.weight', 'backbone.layers.2.blocks.10.attn.qkv.bias', 'backbone.layers.2.blocks.10.attn.proj.weight', 'backbone.layers.2.blocks.10.attn.proj.bias', 'backbone.layers.2.blocks.10.norm2.weight', 'backbone.layers.2.blocks.10.norm2.bias', 'backbone.layers.2.blocks.10.mlp.fc1.weight', 'backbone.layers.2.blocks.10.mlp.fc1.bias', 'backbone.layers.2.blocks.10.mlp.fc2.weight', 'backbone.layers.2.blocks.10.mlp.fc2.bias', 'backbone.layers.2.blocks.11.norm1.weight', 'backbone.layers.2.blocks.11.norm1.bias', 'backbone.layers.2.blocks.11.attn.relative_position_bias_table', 'backbone.layers.2.blocks.11.attn.relative_position_index', 'backbone.layers.2.blocks.11.attn.qkv.weight', 'backbone.layers.2.blocks.11.attn.qkv.bias', 'backbone.layers.2.blocks.11.attn.proj.weight', 'backbone.layers.2.blocks.11.attn.proj.bias', 'backbone.layers.2.blocks.11.norm2.weight', 'backbone.layers.2.blocks.11.norm2.bias', 'backbone.layers.2.blocks.11.mlp.fc1.weight', 'backbone.layers.2.blocks.11.mlp.fc1.bias', 'backbone.layers.2.blocks.11.mlp.fc2.weight', 'backbone.layers.2.blocks.11.mlp.fc2.bias', 'backbone.layers.2.blocks.12.norm1.weight', 'backbone.layers.2.blocks.12.norm1.bias', 'backbone.layers.2.blocks.12.attn.relative_position_bias_table', 'backbone.layers.2.blocks.12.attn.relative_position_index', 'backbone.layers.2.blocks.12.attn.qkv.weight', 'backbone.layers.2.blocks.12.attn.qkv.bias', 'backbone.layers.2.blocks.12.attn.proj.weight', 'backbone.layers.2.blocks.12.attn.proj.bias', 'backbone.layers.2.blocks.12.norm2.weight', 'backbone.layers.2.blocks.12.norm2.bias', 'backbone.layers.2.blocks.12.mlp.fc1.weight', 'backbone.layers.2.blocks.12.mlp.fc1.bias', 'backbone.layers.2.blocks.12.mlp.fc2.weight', 'backbone.layers.2.blocks.12.mlp.fc2.bias', 'backbone.layers.2.blocks.13.norm1.weight', 'backbone.layers.2.blocks.13.norm1.bias', 'backbone.layers.2.blocks.13.attn.relative_position_bias_table', 'backbone.layers.2.blocks.13.attn.relative_position_index', 'backbone.layers.2.blocks.13.attn.qkv.weight', 'backbone.layers.2.blocks.13.attn.qkv.bias', 'backbone.layers.2.blocks.13.attn.proj.weight', 'backbone.layers.2.blocks.13.attn.proj.bias', 'backbone.layers.2.blocks.13.norm2.weight', 'backbone.layers.2.blocks.13.norm2.bias', 'backbone.layers.2.blocks.13.mlp.fc1.weight', 'backbone.layers.2.blocks.13.mlp.fc1.bias', 'backbone.layers.2.blocks.13.mlp.fc2.weight', 'backbone.layers.2.blocks.13.mlp.fc2.bias', 'backbone.layers.2.blocks.14.norm1.weight', 'backbone.layers.2.blocks.14.norm1.bias', 'backbone.layers.2.blocks.14.attn.relative_position_bias_table', 'backbone.layers.2.blocks.14.attn.relative_position_index', 'backbone.layers.2.blocks.14.attn.qkv.weight', 'backbone.layers.2.blocks.14.attn.qkv.bias', 'backbone.layers.2.blocks.14.attn.proj.weight', 'backbone.layers.2.blocks.14.attn.proj.bias', 'backbone.layers.2.blocks.14.norm2.weight', 'backbone.layers.2.blocks.14.norm2.bias', 'backbone.layers.2.blocks.14.mlp.fc1.weight', 'backbone.layers.2.blocks.14.mlp.fc1.bias', 'backbone.layers.2.blocks.14.mlp.fc2.weight', 'backbone.layers.2.blocks.14.mlp.fc2.bias', 'backbone.layers.2.blocks.15.norm1.weight', 'backbone.layers.2.blocks.15.norm1.bias', 'backbone.layers.2.blocks.15.attn.relative_position_bias_table', 'backbone.layers.2.blocks.15.attn.relative_position_index', 'backbone.layers.2.blocks.15.attn.qkv.weight', 'backbone.layers.2.blocks.15.attn.qkv.bias', 'backbone.layers.2.blocks.15.attn.proj.weight', 'backbone.layers.2.blocks.15.attn.proj.bias', 'backbone.layers.2.blocks.15.norm2.weight', 'backbone.layers.2.blocks.15.norm2.bias', 'backbone.layers.2.blocks.15.mlp.fc1.weight', 'backbone.layers.2.blocks.15.mlp.fc1.bias', 'backbone.layers.2.blocks.15.mlp.fc2.weight', 'backbone.layers.2.blocks.15.mlp.fc2.bias', 'backbone.layers.2.blocks.16.norm1.weight', 'backbone.layers.2.blocks.16.norm1.bias', 'backbone.layers.2.blocks.16.attn.relative_position_bias_table', 'backbone.layers.2.blocks.16.attn.relative_position_index', 'backbone.layers.2.blocks.16.attn.qkv.weight', 'backbone.layers.2.blocks.16.attn.qkv.bias', 'backbone.layers.2.blocks.16.attn.proj.weight', 'backbone.layers.2.blocks.16.attn.proj.bias', 'backbone.layers.2.blocks.16.norm2.weight', 'backbone.layers.2.blocks.16.norm2.bias', 'backbone.layers.2.blocks.16.mlp.fc1.weight', 'backbone.layers.2.blocks.16.mlp.fc1.bias', 'backbone.layers.2.blocks.16.mlp.fc2.weight', 'backbone.layers.2.blocks.16.mlp.fc2.bias', 'backbone.layers.2.blocks.17.norm1.weight', 'backbone.layers.2.blocks.17.norm1.bias', 'backbone.layers.2.blocks.17.attn.relative_position_bias_table', 'backbone.layers.2.blocks.17.attn.relative_position_index', 'backbone.layers.2.blocks.17.attn.qkv.weight', 'backbone.layers.2.blocks.17.attn.qkv.bias', 'backbone.layers.2.blocks.17.attn.proj.weight', 'backbone.layers.2.blocks.17.attn.proj.bias', 'backbone.layers.2.blocks.17.norm2.weight', 'backbone.layers.2.blocks.17.norm2.bias', 'backbone.layers.2.blocks.17.mlp.fc1.weight', 'backbone.layers.2.blocks.17.mlp.fc1.bias', 'backbone.layers.2.blocks.17.mlp.fc2.weight', 'backbone.layers.2.blocks.17.mlp.fc2.bias', 'backbone.layers.2.downsample.reduction.weight', 'backbone.layers.2.downsample.norm.weight', 'backbone.layers.2.downsample.norm.bias', 'backbone.layers.3.blocks.0.norm1.weight', 'backbone.layers.3.blocks.0.norm1.bias', 'backbone.layers.3.blocks.0.attn.relative_position_bias_table', 'backbone.layers.3.blocks.0.attn.relative_position_index', 'backbone.layers.3.blocks.0.attn.qkv.weight', 'backbone.layers.3.blocks.0.attn.qkv.bias', 'backbone.layers.3.blocks.0.attn.proj.weight', 'backbone.layers.3.blocks.0.attn.proj.bias', 'backbone.layers.3.blocks.0.norm2.weight', 'backbone.layers.3.blocks.0.norm2.bias', 'backbone.layers.3.blocks.0.mlp.fc1.weight', 'backbone.layers.3.blocks.0.mlp.fc1.bias', 'backbone.layers.3.blocks.0.mlp.fc2.weight', 'backbone.layers.3.blocks.0.mlp.fc2.bias', 'backbone.layers.3.blocks.1.norm1.weight', 'backbone.layers.3.blocks.1.norm1.bias', 'backbone.layers.3.blocks.1.attn.relative_position_bias_table', 'backbone.layers.3.blocks.1.attn.relative_position_index', 'backbone.layers.3.blocks.1.attn.qkv.weight', 'backbone.layers.3.blocks.1.attn.qkv.bias', 'backbone.layers.3.blocks.1.attn.proj.weight', 'backbone.layers.3.blocks.1.attn.proj.bias', 'backbone.layers.3.blocks.1.norm2.weight', 'backbone.layers.3.blocks.1.norm2.bias', 'backbone.layers.3.blocks.1.mlp.fc1.weight', 'backbone.layers.3.blocks.1.mlp.fc1.bias', 'backbone.layers.3.blocks.1.mlp.fc2.weight', 'backbone.layers.3.blocks.1.mlp.fc2.bias', 'backbone.norm0.weight', 'backbone.norm0.bias', 'backbone.norm1.weight', 'backbone.norm1.bias', 'backbone.norm2.weight', 'backbone.norm2.bias', 'backbone.norm3.weight', 'backbone.norm3.bias', 'sem_seg_head.pixel_decoder.adapter_1.weight', 'sem_seg_head.pixel_decoder.adapter_1.norm.weight', 'sem_seg_head.pixel_decoder.adapter_1.norm.bias', 'sem_seg_head.pixel_decoder.layer_1.weight', 'sem_seg_head.pixel_decoder.layer_1.norm.weight', 'sem_seg_head.pixel_decoder.layer_1.norm.bias', 'sem_seg_head.pixel_decoder.adapter_2.weight', 'sem_seg_head.pixel_decoder.adapter_2.norm.weight', 'sem_seg_head.pixel_decoder.adapter_2.norm.bias', 'sem_seg_head.pixel_decoder.layer_2.weight', 'sem_seg_head.pixel_decoder.layer_2.norm.weight', 'sem_seg_head.pixel_decoder.layer_2.norm.bias', 'sem_seg_head.pixel_decoder.adapter_3.weight', 'sem_seg_head.pixel_decoder.adapter_3.norm.weight', 'sem_seg_head.pixel_decoder.adapter_3.norm.bias', 'sem_seg_head.pixel_decoder.layer_3.weight', 'sem_seg_head.pixel_decoder.layer_3.norm.weight', 'sem_seg_head.pixel_decoder.layer_3.norm.bias', 'sem_seg_head.pixel_decoder.layer_4.weight', 'sem_seg_head.pixel_decoder.layer_4.norm.weight', 'sem_seg_head.pixel_decoder.layer_4.norm.bias', 'sem_seg_head.pixel_decoder.mask_features.weight', 'sem_seg_head.pixel_decoder.mask_features.bias', 'sem_seg_head.predictor.transformer.decoder.layers.0.self_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.0.self_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.0.self_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.0.self_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.0.linear1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.0.linear1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.0.linear2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.0.linear2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.0.norm1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.0.norm1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.0.norm2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.0.norm2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.0.norm3.weight', 'sem_seg_head.predictor.transformer.decoder.layers.0.norm3.bias', 'sem_seg_head.predictor.transformer.decoder.layers.1.self_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.1.self_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.1.self_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.1.self_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.1.linear1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.1.linear1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.1.linear2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.1.linear2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.1.norm1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.1.norm1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.1.norm2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.1.norm2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.1.norm3.weight', 'sem_seg_head.predictor.transformer.decoder.layers.1.norm3.bias', 'sem_seg_head.predictor.transformer.decoder.layers.2.self_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.2.self_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.2.self_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.2.self_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.2.linear1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.2.linear1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.2.linear2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.2.linear2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.2.norm1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.2.norm1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.2.norm2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.2.norm2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.2.norm3.weight', 'sem_seg_head.predictor.transformer.decoder.layers.2.norm3.bias', 'sem_seg_head.predictor.transformer.decoder.layers.3.self_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.3.self_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.3.self_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.3.self_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.3.linear1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.3.linear1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.3.linear2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.3.linear2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.3.norm1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.3.norm1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.3.norm2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.3.norm2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.3.norm3.weight', 'sem_seg_head.predictor.transformer.decoder.layers.3.norm3.bias', 'sem_seg_head.predictor.transformer.decoder.layers.4.self_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.4.self_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.4.self_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.4.self_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.4.multihead_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.4.multihead_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.4.multihead_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.4.multihead_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.4.linear1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.4.linear1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.4.linear2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.4.linear2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.4.norm1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.4.norm1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.4.norm2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.4.norm2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.4.norm3.weight', 'sem_seg_head.predictor.transformer.decoder.layers.4.norm3.bias', 'sem_seg_head.predictor.transformer.decoder.layers.5.self_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.5.self_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.5.self_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.5.self_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.5.multihead_attn.in_proj_weight', 'sem_seg_head.predictor.transformer.decoder.layers.5.multihead_attn.in_proj_bias', 'sem_seg_head.predictor.transformer.decoder.layers.5.multihead_attn.out_proj.weight', 'sem_seg_head.predictor.transformer.decoder.layers.5.multihead_attn.out_proj.bias', 'sem_seg_head.predictor.transformer.decoder.layers.5.linear1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.5.linear1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.5.linear2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.5.linear2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.5.norm1.weight', 'sem_seg_head.predictor.transformer.decoder.layers.5.norm1.bias', 'sem_seg_head.predictor.transformer.decoder.layers.5.norm2.weight', 'sem_seg_head.predictor.transformer.decoder.layers.5.norm2.bias', 'sem_seg_head.predictor.transformer.decoder.layers.5.norm3.weight', 'sem_seg_head.predictor.transformer.decoder.layers.5.norm3.bias', 'sem_seg_head.predictor.transformer.decoder.norm.weight', 'sem_seg_head.predictor.transformer.decoder.norm.bias', 'sem_seg_head.predictor.query_embed.weight', 'sem_seg_head.predictor.input_proj.weight', 'sem_seg_head.predictor.input_proj.bias', 'sem_seg_head.predictor.mask_embed.layers.0.weight', 'sem_seg_head.predictor.mask_embed.layers.0.bias', 'sem_seg_head.predictor.mask_embed.layers.1.weight', 'sem_seg_head.predictor.mask_embed.layers.1.bias', 'sem_seg_head.predictor.mask_embed.layers.2.weight', 'sem_seg_head.predictor.mask_embed.layers.2.bias', 'sem_seg_head.predictor.class_embed.layers.0.weight', 'sem_seg_head.predictor.class_embed.layers.0.bias', 'sem_seg_head.predictor.class_embed.layers.1.weight', 'sem_seg_head.predictor.class_embed.layers.1.bias', 'criterion.empty_weight', 'clip_adapter.non_object_embedding', 'clip_adapter.pixel_mean', 'clip_adapter.pixel_std', 'clip_adapter.clip_model.positional_embedding', 'clip_adapter.clip_model.text_projection', 'clip_adapter.clip_model.logit_scale', 'clip_adapter.clip_model.visual.class_embedding', 'clip_adapter.clip_model.visual.positional_embedding', 'clip_adapter.clip_model.visual.proj', 'clip_adapter.clip_model.visual.conv1.weight', 'clip_adapter.clip_model.visual.ln_pre.weight', 'clip_adapter.clip_model.visual.ln_pre.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.0.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.0.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.0.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.0.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.0.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.0.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.1.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.1.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.1.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.1.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.1.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.1.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.1.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.2.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.2.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.2.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.2.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.2.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.2.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.2.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.2.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.3.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.3.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.3.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.3.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.3.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.3.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.3.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.3.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.4.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.4.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.4.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.4.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.4.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.4.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.4.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.4.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.5.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.5.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.5.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.5.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.5.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.5.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.5.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.5.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.6.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.6.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.6.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.6.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.6.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.6.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.6.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.6.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.7.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.7.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.7.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.7.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.7.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.7.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.7.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.7.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.8.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.8.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.8.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.8.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.8.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.8.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.8.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.8.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.9.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.9.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.9.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.9.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.9.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.9.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.9.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.9.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.10.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.10.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.10.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.10.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.11.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.11.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.11.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.11.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.12.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.12.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.12.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.12.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.12.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.12.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.12.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.12.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.12.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.12.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.12.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.12.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.13.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.13.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.13.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.13.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.13.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.13.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.13.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.13.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.13.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.13.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.13.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.13.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.14.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.14.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.14.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.14.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.14.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.14.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.14.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.14.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.14.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.14.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.14.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.14.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.15.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.15.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.15.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.15.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.15.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.15.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.15.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.15.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.15.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.15.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.15.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.15.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.16.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.16.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.16.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.16.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.16.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.16.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.16.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.16.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.16.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.16.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.16.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.16.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.17.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.17.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.17.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.17.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.17.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.17.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.17.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.17.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.17.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.17.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.17.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.17.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.18.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.18.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.18.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.18.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.18.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.18.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.18.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.18.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.18.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.18.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.18.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.18.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.19.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.19.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.19.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.19.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.19.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.19.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.19.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.19.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.19.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.19.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.19.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.19.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.20.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.20.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.20.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.20.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.20.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.20.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.20.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.20.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.20.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.20.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.20.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.20.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.21.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.21.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.21.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.21.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.21.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.21.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.21.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.21.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.21.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.21.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.21.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.21.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.22.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.22.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.22.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.22.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.22.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.22.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.22.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.22.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.22.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.22.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.22.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.22.ln_2.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.23.attn.in_proj_weight', 'clip_adapter.clip_model.visual.transformer.resblocks.23.attn.in_proj_bias', 'clip_adapter.clip_model.visual.transformer.resblocks.23.attn.out_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.23.attn.out_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.23.ln_1.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.23.ln_1.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.23.mlp.c_fc.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.23.mlp.c_fc.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.23.mlp.c_proj.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.23.mlp.c_proj.bias', 'clip_adapter.clip_model.visual.transformer.resblocks.23.ln_2.weight', 'clip_adapter.clip_model.visual.transformer.resblocks.23.ln_2.bias', 'clip_adapter.clip_model.visual.ln_post.weight', 'clip_adapter.clip_model.visual.ln_post.bias', 'clip_adapter.clip_model.transformer.resblocks.0.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.0.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.0.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.0.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.0.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.0.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.0.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.0.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.0.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.0.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.0.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.0.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.1.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.1.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.1.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.1.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.1.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.1.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.1.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.1.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.1.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.1.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.1.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.1.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.2.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.2.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.2.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.2.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.2.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.2.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.2.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.2.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.2.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.2.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.2.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.2.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.3.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.3.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.3.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.3.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.3.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.3.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.3.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.3.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.3.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.3.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.3.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.3.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.4.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.4.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.4.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.4.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.4.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.4.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.4.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.4.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.4.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.4.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.4.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.4.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.5.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.5.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.5.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.5.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.5.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.5.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.5.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.5.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.5.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.5.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.5.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.5.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.6.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.6.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.6.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.6.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.6.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.6.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.6.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.6.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.6.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.6.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.6.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.6.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.7.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.7.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.7.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.7.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.7.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.7.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.7.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.7.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.7.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.7.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.7.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.7.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.8.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.8.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.8.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.8.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.8.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.8.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.8.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.8.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.8.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.8.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.8.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.8.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.9.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.9.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.9.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.9.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.9.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.9.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.9.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.9.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.9.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.9.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.9.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.9.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.10.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.10.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.10.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.10.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.10.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.10.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.10.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.10.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.10.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.10.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.10.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.10.ln_2.bias', 'clip_adapter.clip_model.transformer.resblocks.11.attn.in_proj_weight', 'clip_adapter.clip_model.transformer.resblocks.11.attn.in_proj_bias', 'clip_adapter.clip_model.transformer.resblocks.11.attn.out_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.11.attn.out_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.11.ln_1.weight', 'clip_adapter.clip_model.transformer.resblocks.11.ln_1.bias', 'clip_adapter.clip_model.transformer.resblocks.11.mlp.c_fc.weight', 'clip_adapter.clip_model.transformer.resblocks.11.mlp.c_fc.bias', 'clip_adapter.clip_model.transformer.resblocks.11.mlp.c_proj.weight', 'clip_adapter.clip_model.transformer.resblocks.11.mlp.c_proj.bias', 'clip_adapter.clip_model.transformer.resblocks.11.ln_2.weight', 'clip_adapter.clip_model.transformer.resblocks.11.ln_2.bias', 'clip_adapter.clip_model.token_embedding.weight', 'clip_adapter.clip_model.ln_final.weight', 'clip_adapter.clip_model.ln_final.bias', 'clip_adapter.clip_model.visual.mask_embedding'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SwinTransformer(embed_dim=128,depths=[2,2,18,2],num_heads=[4,8,16,32],window_size=12,ape=False,drop_path_rate=0.3,patch_norm=True,pretrain_img_size=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "backbone_checkpoint = OrderedDict()\n",
    "for f in checkpoint['model'].items():\n",
    "    name = f[0]\n",
    "    tensor = f[1]\n",
    "    if \"backbone\" in name:\n",
    "        name_new = name.replace(\"backbone.\",\"\")\n",
    "        backbone_checkpoint[name_new] = tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(backbone_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"/workspace/data/ADE20K_2021_17_01/images/ADE/training/cultural/aquarium/ADE_frame_00000007.JPG\")\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "H, W = 224, 224\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "tensor = transforms(img)\n",
    "tensor = torch.unsqueeze(tensor,0)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
